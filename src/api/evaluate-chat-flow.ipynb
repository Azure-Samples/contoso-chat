{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import prompty\n",
    "from evaluators.custom_evals.coherence import coherence_evaluation\n",
    "from evaluators.custom_evals.relevance import relevance_evaluation\n",
    "from evaluators.custom_evals.fluency import fluency_evaluation\n",
    "from evaluators.custom_evals.groundedness import groundedness_evaluation\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "from contoso_chat.chat_request import get_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get output from data and save to results jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data_path = \"./evaluators/data.jsonl\"\n",
    "\n",
    "    df = pd.read_json(data_path, lines=True)\n",
    "    df.head()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_response_data(df):\n",
    "    results = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        customerId = row['customerId']\n",
    "        question = row['question']\n",
    "        \n",
    "        # Run contoso-chat/chat_request flow to get response\n",
    "        response = get_response(customerId=customerId, question=question, chat_history=[])\n",
    "        print(response)\n",
    "        \n",
    "        # Add results to list\n",
    "        result = {\n",
    "            'question': question,\n",
    "            'context': response[\"context\"],\n",
    "            'answer': response[\"answer\"]\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Save results to a JSONL file\n",
    "    with open('result.jsonl', 'w') as file:\n",
    "        for result in results:\n",
    "            file.write(json.dumps(result) + '\\n')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    # Evaluate results from results file\n",
    "    results_path = 'result.jsonl'\n",
    "    results = []\n",
    "    with open(results_path, 'r') as file:\n",
    "        for line in file:\n",
    "            print(line)\n",
    "            results.append(json.loads(line))\n",
    "\n",
    "    for result in results:\n",
    "        question = result['question']\n",
    "        context = result['context']\n",
    "        answer = result['answer']\n",
    "        \n",
    "        groundedness_score = groundedness_evaluation(question=question, answer=answer, context=context)\n",
    "        fluency_score = fluency_evaluation(question=question, answer=answer, context=context)\n",
    "        coherence_score = coherence_evaluation(question=question, answer=answer, context=context)\n",
    "        relevance_score = relevance_evaluation(question=question, answer=answer, context=context)\n",
    "        \n",
    "        result['groundedness'] = groundedness_score\n",
    "        result['fluency'] = fluency_score\n",
    "        result['coherence'] = coherence_score\n",
    "        result['relevance'] = relevance_score\n",
    "\n",
    "    # Save results to a JSONL file\n",
    "    with open('result_evaluated.jsonl', 'w') as file:\n",
    "        for result in results:\n",
    "            file.write(json.dumps(result) + '\\n')\n",
    "\n",
    "    with jsonlines.open('eval_results.jsonl', 'w') as writer:\n",
    "        writer.write(results)\n",
    "    # Print results\n",
    "\n",
    "    df = pd.read_json('result_evaluated.jsonl', lines=True)\n",
    "    df.head()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary(df):\n",
    "    print(\"Evaluation summary:\\n\")\n",
    "    print(df)\n",
    "    # drop question, context and answer\n",
    "    mean_df = df.drop([\"question\", \"context\", \"answer\"], axis=1).mean()\n",
    "    print(\"\\nAverage scores:\")\n",
    "    print(mean_df)\n",
    "    df.to_markdown('eval_results.md')\n",
    "    with open('eval_results.md', 'a') as file:\n",
    "        file.write(\"\\n\\nAverages scores:\\n\\n\")\n",
    "    mean_df.to_markdown('eval_results.md', 'a')\n",
    "\n",
    "    print(\"Results saved to result_evaluated.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting get_response\n",
      "signature:\n",
      "\"contoso_chat.chat_request.get_response\"\n",
      "inputs:\n",
      "{\n",
      "    \"customerId\": 4,\n",
      "    \"question\": \"tell me about your hiking jackets\",\n",
      "    \"chat_history\": []\n",
      "}\n",
      "getting customer...\n",
      "Starting get_customer\n",
      "signature:\n",
      "\"contoso_chat.chat_request.get_customer\"\n",
      "inputs:\n",
      "{\n",
      "    \"customerId\": 4\n",
      "}\n",
      "result:\n",
      "{\n",
      "    \"id\": \"4\",\n",
      "    \"firstName\": \"Sarah\",\n",
      "    \"lastName\": \"Lee\",\n",
      "    \"age\": 38,\n",
      "    \"email\": \"sarahlee@example.com\",\n",
      "    \"phone\": \"555-867-5309\",\n",
      "    \"address\": \"321 Maple St, Bigtown USA, 90123\",\n",
      "    \"membership\": \"Platinum\",\n",
      "    \"orders\": [\n",
      "        {\n",
      "            \"id\": 26,\n",
      "            \"productId\": 7,\n",
      "            \"quantity\": 1,\n",
      "            \"total\": 100,\n",
      "            \"date\": \"2/5/2023\",\n",
      "            \"name\": \"CozyNights Sleeping Bag\",\n",
      "            \"unitprice\": 100,\n",
      "            \"category\": \"Sleeping Bags\",\n",
      "            \"brand\": \"CozyNights\",\n",
      "            \"description\": \"Embrace the great outdoors in any season with the lightweight CozyNights Sleeping Bag! This durable three-season bag is superbly designed to give hikers, campers, and backpackers comfort and warmth during spring, summer, and fall. With a compact design that folds down into a convenient stuff sack, you can whisk it away on any adventure without a hitch. The sleeping bag takes comfort seriously, featuring a handy hood, ample room and padding, and a reliable temperature rating. Crafted from high-quality polyester, it ensures long-lasting use and can even be zipped together with another bag for shared comfort. Whether you're gazing at stars or catching a quick nap between trails, the CozyNights Sleeping Bag makes it a treat. Don't just sleep\\u2014 dream with CozyNights.\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 35,\n",
      "            \"productId\": 10,\n",
      "            \"quantity\": 1,\n",
      "            \"total\": 75,\n",
      "            \"date\": \"2/20/2023\",\n",
      "            \"name\": \"TrailBlaze Hiking Pants\",\n",
      "            \"unitprice\": 75,\n",
      "            \"category\": \"Hiking Clothing\",\n",
      "            \"brand\": \"MountainStyle\",\n",
      "            \"description\": \"Meet the TrailBlaze Hiking Pants from MountainStyle, the stylish khaki champions of the trails. These are not just pants; they're your passport to outdoor adventure. Crafted from high-quality nylon fabric, these dapper troopers are lightweight and fast-drying, with a water-resistant armor that laughs off light rain. Their breathable design whisks away sweat while their articulated knees grant you the flexibility of a mountain goat. Zippered pockets guard your essentials, making them a hiker's best ally. Designed with durability for all your trekking trials, these pants come with a comfortable, ergonomic fit that will make you forget you're wearing them. Sneak a peek, and you are sure to be tempted by the sleek allure that is the TrailBlaze Hiking Pants. Your outdoors wardrobe wouldn't be quite complete without them.\"\n",
      "        }\n",
      "    ],\n",
      "    \"_rid\": \"Uz4mAP4eaGIMAAAAAAAAAA==\",\n",
      "    \"_self\": \"dbs/Uz4mAA==/colls/Uz4mAP4eaGI=/docs/Uz4mAP4eaGIMAAAAAAAAAA==/\",\n",
      "    \"_etag\": \"\\\"00007fbe-0000-0100-0000-678fb1e00000\\\"\",\n",
      "    \"_attachments\": \"attachments/\",\n",
      "    \"_ts\": 1737470432\n",
      "}\n",
      "Ending get_customer\n",
      "customer complete\n",
      "Starting execute\n",
      "signature:\n",
      "\"prompty.execute\"\n",
      "description:\n",
      "\"Execute a prompty\"\n",
      "inputs:\n",
      "{\n",
      "    \"prompt\": \"product.prompty\",\n",
      "    \"configuration\": {\n",
      "        \"azure_endpoint\": \"https://aoai-pghtmg6d4ebua.openai.azure.com/\",\n",
      "        \"api_version\": \"2024-07-18\"\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"inputs\": {\n",
      "        \"context\": \"tell me about your hiking jackets\"\n",
      "    },\n",
      "    \"raw\": false,\n",
      "    \"config_name\": \"default\"\n",
      "}\n",
      "Starting load\n",
      "signature:\n",
      "\"prompty.load\"\n",
      "description:\n",
      "\"Load a prompty file.\"\n",
      "inputs:\n",
      "{\n",
      "    \"prompty_file\": \"/workspaces/contoso-chat/src/api/contoso_chat/product/product.prompty\",\n",
      "    \"configuration\": \"default\"\n",
      "}\n",
      "result:\n",
      "{\n",
      "    \"name\": \"Contoso Product Reasearch\",\n",
      "    \"description\": \"A prompt that uses context to ground an incoming question\",\n",
      "    \"authors\": [\n",
      "        \"Seth Juarez\"\n",
      "    ],\n",
      "    \"model\": {\n",
      "        \"api\": \"chat\",\n",
      "        \"configuration\": {\n",
      "            \"type\": \"azure_openai\",\n",
      "            \"azure_deployment\": \"gpt-4o-mini\",\n",
      "            \"api_version\": \"2024-07-18\",\n",
      "            \"azure_endpoint\": \"AZURE_OPENAI_ENDPOINT\"\n",
      "        },\n",
      "        \"parameters\": {\n",
      "            \"max_tokens\": 1500\n",
      "        },\n",
      "        \"response\": {}\n",
      "    },\n",
      "    \"sample\": {\n",
      "        \"context\": \"Can you use a selection of sports and outdoor cooking gear as context?\"\n",
      "    },\n",
      "    \"template\": {\n",
      "        \"type\": \"jinja2\",\n",
      "        \"parser\": \"prompty\"\n",
      "    },\n",
      "    \"file\": \"/workspaces/contoso-chat/src/api/contoso_chat/product/product.prompty\",\n",
      "    \"content\": \"system:\\n\\nYou are an AI assistant who helps people find information from a search index.\\nYou can take context and create number of specialized queries to make to the\\nsearch index return the most relevant information for a writer to use when\\nwriting marketing articles.\\n\\n# Context\\nUse the follow contex to provide a set of specialized queries to the search index:\\n\\n{{context}}\\n\\n# Response format\\nThe response format is a JSON array that contains a list of specialized queries \\nto make to the search index. Here is an example:\\n\\ncontext: Can you find a selection of outdoor apparel?\\nqueries:\\n[\\n  \\\"outdoor apparel\\\",\\n  \\\"outdoor clothing\\\",\\n  \\\"outdoor gear\\\",\\n  \\\"outdoor clothing brands\\\",\\n  \\\"outdoor clothing stores\\\",\\n]\\n\\nThis only an example of the output structure. You should make sure to use the context the user gives you\\nto generate the queries.\\n\\n# Output format\\nOnly output the full array of specialized queries to make to the search index. Limit\\nyoursef to 5 queries.\\n\\nuser:\\n{{context}}\\n\"\n",
      "}\n",
      "Ending load\n",
      "Starting prepare\n",
      "signature:\n",
      "\"prompty.prepare\"\n",
      "description:\n",
      "\"Prepare the inputs for the prompt.\"\n",
      "inputs:\n",
      "{\n",
      "    \"prompt\": {\n",
      "        \"name\": \"Contoso Product Reasearch\",\n",
      "        \"description\": \"A prompt that uses context to ground an incoming question\",\n",
      "        \"authors\": [\n",
      "            \"Seth Juarez\"\n",
      "        ],\n",
      "        \"model\": {\n",
      "            \"api\": \"chat\",\n",
      "            \"configuration\": {\n",
      "                \"type\": \"azure_openai\",\n",
      "                \"azure_deployment\": \"gpt-4o-mini\",\n",
      "                \"api_version\": \"2024-07-18\",\n",
      "                \"azure_endpoint\": \"AZURE_OPENAI_ENDPOINT\"\n",
      "            },\n",
      "            \"parameters\": {\n",
      "                \"max_tokens\": 1500\n",
      "            },\n",
      "            \"response\": {}\n",
      "        },\n",
      "        \"sample\": {\n",
      "            \"context\": \"Can you use a selection of sports and outdoor cooking gear as context?\"\n",
      "        },\n",
      "        \"template\": {\n",
      "            \"type\": \"jinja2\",\n",
      "            \"parser\": \"prompty\"\n",
      "        },\n",
      "        \"file\": \"/workspaces/contoso-chat/src/api/contoso_chat/product/product.prompty\",\n",
      "        \"content\": \"system:\\n\\nYou are an AI assistant who helps people find information from a search index.\\nYou can take context and create number of specialized queries to make to the\\nsearch index return the most relevant information for a writer to use when\\nwriting marketing articles.\\n\\n# Context\\nUse the follow contex to provide a set of specialized queries to the search index:\\n\\n{{context}}\\n\\n# Response format\\nThe response format is a JSON array that contains a list of specialized queries \\nto make to the search index. Here is an example:\\n\\ncontext: Can you find a selection of outdoor apparel?\\nqueries:\\n[\\n  \\\"outdoor apparel\\\",\\n  \\\"outdoor clothing\\\",\\n  \\\"outdoor gear\\\",\\n  \\\"outdoor clothing brands\\\",\\n  \\\"outdoor clothing stores\\\",\\n]\\n\\nThis only an example of the output structure. You should make sure to use the context the user gives you\\nto generate the queries.\\n\\n# Output format\\nOnly output the full array of specialized queries to make to the search index. Limit\\nyoursef to 5 queries.\\n\\nuser:\\n{{context}}\\n\"\n",
      "    },\n",
      "    \"inputs\": {\n",
      "        \"context\": \"tell me about your hiking jackets\"\n",
      "    }\n",
      "}\n",
      "Starting Jinja2Renderer\n",
      "signature:\n",
      "\"prompty.renderers.Jinja2Renderer.invoke\"\n",
      "inputs:\n",
      "{\n",
      "    \"data\": {\n",
      "        \"context\": \"tell me about your hiking jackets\"\n",
      "    }\n",
      "}\n",
      "result:\n",
      "\"system:\\n\\nYou are an AI assistant who helps people find information from a search index.\\nYou can take context and create number of specialized queries to make to the\\nsearch index return the most relevant information for a writer to use when\\nwriting marketing articles.\\n\\n# Context\\nUse the follow contex to provide a set of specialized queries to the search index:\\n\\ntell me about your hiking jackets\\n\\n# Response format\\nThe response format is a JSON array that contains a list of specialized queries \\nto make to the search index. Here is an example:\\n\\ncontext: Can you find a selection of outdoor apparel?\\nqueries:\\n[\\n  \\\"outdoor apparel\\\",\\n  \\\"outdoor clothing\\\",\\n  \\\"outdoor gear\\\",\\n  \\\"outdoor clothing brands\\\",\\n  \\\"outdoor clothing stores\\\",\\n]\\n\\nThis only an example of the output structure. You should make sure to use the context the user gives you\\nto generate the queries.\\n\\n# Output format\\nOnly output the full array of specialized queries to make to the search index. Limit\\nyoursef to 5 queries.\\n\\nuser:\\ntell me about your hiking jackets\"\n",
      "Ending Jinja2Renderer\n",
      "Starting PromptyChatParser\n",
      "signature:\n",
      "\"prompty.parsers.PromptyChatParser.invoke\"\n",
      "inputs:\n",
      "{\n",
      "    \"data\": \"system:\\n\\nYou are an AI assistant who helps people find information from a search index.\\nYou can take context and create number of specialized queries to make to the\\nsearch index return the most relevant information for a writer to use when\\nwriting marketing articles.\\n\\n# Context\\nUse the follow contex to provide a set of specialized queries to the search index:\\n\\ntell me about your hiking jackets\\n\\n# Response format\\nThe response format is a JSON array that contains a list of specialized queries \\nto make to the search index. Here is an example:\\n\\ncontext: Can you find a selection of outdoor apparel?\\nqueries:\\n[\\n  \\\"outdoor apparel\\\",\\n  \\\"outdoor clothing\\\",\\n  \\\"outdoor gear\\\",\\n  \\\"outdoor clothing brands\\\",\\n  \\\"outdoor clothing stores\\\",\\n]\\n\\nThis only an example of the output structure. You should make sure to use the context the user gives you\\nto generate the queries.\\n\\n# Output format\\nOnly output the full array of specialized queries to make to the search index. Limit\\nyoursef to 5 queries.\\n\\nuser:\\ntell me about your hiking jackets\"\n",
      "}\n",
      "result:\n",
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are an AI assistant who helps people find information from a search index.\\nYou can take context and create number of specialized queries to make to the\\nsearch index return the most relevant information for a writer to use when\\nwriting marketing articles.\\n\\n# Context\\nUse the follow contex to provide a set of specialized queries to the search index:\\n\\ntell me about your hiking jackets\\n\\n# Response format\\nThe response format is a JSON array that contains a list of specialized queries \\nto make to the search index. Here is an example:\\n\\ncontext: Can you find a selection of outdoor apparel?\\nqueries:\\n[\\n  \\\"outdoor apparel\\\",\\n  \\\"outdoor clothing\\\",\\n  \\\"outdoor gear\\\",\\n  \\\"outdoor clothing brands\\\",\\n  \\\"outdoor clothing stores\\\",\\n]\\n\\nThis only an example of the output structure. You should make sure to use the context the user gives you\\nto generate the queries.\\n\\n# Output format\\nOnly output the full array of specialized queries to make to the search index. Limit\\nyoursef to 5 queries.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"tell me about your hiking jackets\"\n",
      "    }\n",
      "]\n",
      "Ending PromptyChatParser\n",
      "result:\n",
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are an AI assistant who helps people find information from a search index.\\nYou can take context and create number of specialized queries to make to the\\nsearch index return the most relevant information for a writer to use when\\nwriting marketing articles.\\n\\n# Context\\nUse the follow contex to provide a set of specialized queries to the search index:\\n\\ntell me about your hiking jackets\\n\\n# Response format\\nThe response format is a JSON array that contains a list of specialized queries \\nto make to the search index. Here is an example:\\n\\ncontext: Can you find a selection of outdoor apparel?\\nqueries:\\n[\\n  \\\"outdoor apparel\\\",\\n  \\\"outdoor clothing\\\",\\n  \\\"outdoor gear\\\",\\n  \\\"outdoor clothing brands\\\",\\n  \\\"outdoor clothing stores\\\",\\n]\\n\\nThis only an example of the output structure. You should make sure to use the context the user gives you\\nto generate the queries.\\n\\n# Output format\\nOnly output the full array of specialized queries to make to the search index. Limit\\nyoursef to 5 queries.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"tell me about your hiking jackets\"\n",
      "    }\n",
      "]\n",
      "Ending prepare\n",
      "Starting run\n",
      "signature:\n",
      "\"prompty.run\"\n",
      "description:\n",
      "\"Run the prepared Prompty content against the model.\"\n",
      "inputs:\n",
      "{\n",
      "    \"prompt\": {\n",
      "        \"name\": \"Contoso Product Reasearch\",\n",
      "        \"description\": \"A prompt that uses context to ground an incoming question\",\n",
      "        \"authors\": [\n",
      "            \"Seth Juarez\"\n",
      "        ],\n",
      "        \"model\": {\n",
      "            \"api\": \"chat\",\n",
      "            \"configuration\": {\n",
      "                \"type\": \"azure_openai\",\n",
      "                \"azure_deployment\": \"gpt-4o-mini\",\n",
      "                \"api_version\": \"2024-07-18\",\n",
      "                \"azure_endpoint\": \"AZURE_OPENAI_ENDPOINT\"\n",
      "            },\n",
      "            \"parameters\": {\n",
      "                \"max_tokens\": 1500\n",
      "            },\n",
      "            \"response\": {}\n",
      "        },\n",
      "        \"sample\": {\n",
      "            \"context\": \"Can you use a selection of sports and outdoor cooking gear as context?\"\n",
      "        },\n",
      "        \"template\": {\n",
      "            \"type\": \"jinja2\",\n",
      "            \"parser\": \"prompty\"\n",
      "        },\n",
      "        \"file\": \"/workspaces/contoso-chat/src/api/contoso_chat/product/product.prompty\",\n",
      "        \"content\": \"system:\\n\\nYou are an AI assistant who helps people find information from a search index.\\nYou can take context and create number of specialized queries to make to the\\nsearch index return the most relevant information for a writer to use when\\nwriting marketing articles.\\n\\n# Context\\nUse the follow contex to provide a set of specialized queries to the search index:\\n\\n{{context}}\\n\\n# Response format\\nThe response format is a JSON array that contains a list of specialized queries \\nto make to the search index. Here is an example:\\n\\ncontext: Can you find a selection of outdoor apparel?\\nqueries:\\n[\\n  \\\"outdoor apparel\\\",\\n  \\\"outdoor clothing\\\",\\n  \\\"outdoor gear\\\",\\n  \\\"outdoor clothing brands\\\",\\n  \\\"outdoor clothing stores\\\",\\n]\\n\\nThis only an example of the output structure. You should make sure to use the context the user gives you\\nto generate the queries.\\n\\n# Output format\\nOnly output the full array of specialized queries to make to the search index. Limit\\nyoursef to 5 queries.\\n\\nuser:\\n{{context}}\\n\"\n",
      "    },\n",
      "    \"content\": [\n",
      "        {\n",
      "            \"role\": \"system\",\n",
      "            \"content\": \"You are an AI assistant who helps people find information from a search index.\\nYou can take context and create number of specialized queries to make to the\\nsearch index return the most relevant information for a writer to use when\\nwriting marketing articles.\\n\\n# Context\\nUse the follow contex to provide a set of specialized queries to the search index:\\n\\ntell me about your hiking jackets\\n\\n# Response format\\nThe response format is a JSON array that contains a list of specialized queries \\nto make to the search index. Here is an example:\\n\\ncontext: Can you find a selection of outdoor apparel?\\nqueries:\\n[\\n  \\\"outdoor apparel\\\",\\n  \\\"outdoor clothing\\\",\\n  \\\"outdoor gear\\\",\\n  \\\"outdoor clothing brands\\\",\\n  \\\"outdoor clothing stores\\\",\\n]\\n\\nThis only an example of the output structure. You should make sure to use the context the user gives you\\nto generate the queries.\\n\\n# Output format\\nOnly output the full array of specialized queries to make to the search index. Limit\\nyoursef to 5 queries.\"\n",
      "        },\n",
      "        {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": \"tell me about your hiking jackets\"\n",
      "        }\n",
      "    ],\n",
      "    \"configuration\": {\n",
      "        \"azure_endpoint\": \"https://aoai-pghtmg6d4ebua.openai.azure.com/\",\n",
      "        \"api_version\": \"2024-07-18\"\n",
      "    },\n",
      "    \"parameters\": {},\n",
      "    \"raw\": false\n",
      "}\n",
      "Starting AzureOpenAIExecutor\n",
      "signature:\n",
      "\"prompty.azure.executor.AzureOpenAIExecutor.invoke\"\n",
      "inputs:\n",
      "{\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"role\": \"system\",\n",
      "            \"content\": \"You are an AI assistant who helps people find information from a search index.\\nYou can take context and create number of specialized queries to make to the\\nsearch index return the most relevant information for a writer to use when\\nwriting marketing articles.\\n\\n# Context\\nUse the follow contex to provide a set of specialized queries to the search index:\\n\\ntell me about your hiking jackets\\n\\n# Response format\\nThe response format is a JSON array that contains a list of specialized queries \\nto make to the search index. Here is an example:\\n\\ncontext: Can you find a selection of outdoor apparel?\\nqueries:\\n[\\n  \\\"outdoor apparel\\\",\\n  \\\"outdoor clothing\\\",\\n  \\\"outdoor gear\\\",\\n  \\\"outdoor clothing brands\\\",\\n  \\\"outdoor clothing stores\\\",\\n]\\n\\nThis only an example of the output structure. You should make sure to use the context the user gives you\\nto generate the queries.\\n\\n# Output format\\nOnly output the full array of specialized queries to make to the search index. Limit\\nyoursef to 5 queries.\"\n",
      "        },\n",
      "        {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": \"tell me about your hiking jackets\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Starting AzureOpenAI\n",
      "type:\n",
      "\"LLM\"\n",
      "signature:\n",
      "\"AzureOpenAI.ctor\"\n",
      "description:\n",
      "\"Azure OpenAI Constructor\"\n",
      "inputs:\n",
      "{\n",
      "    \"azure_endpoint\": \"https://aoai-pghtmg6d4ebua.openai.azure.com/\",\n",
      "    \"api_version\": \"2024-07-18\",\n",
      "    \"azure_deployment\": \"gpt-4o-mini\",\n",
      "    \"azure_ad_token_provider\": \"***********************************************************************\"\n",
      "}\n",
      "result:\n",
      "\"<openai.lib.azure.AzureOpenAI object at 0x7da9ac1dbc50>\"\n",
      "Ending AzureOpenAI\n",
      "Starting create\n",
      "type:\n",
      "\"LLM\"\n",
      "description:\n",
      "\"Azure OpenAI Client\"\n",
      "signature:\n",
      "\"AzureOpenAI.chat.completions.create\"\n",
      "inputs:\n",
      "{\n",
      "    \"model\": \"gpt-4o-mini\",\n",
      "    \"messages\": [\n",
      "        {\n",
      "            \"role\": \"system\",\n",
      "            \"content\": \"You are an AI assistant who helps people find information from a search index.\\nYou can take context and create number of specialized queries to make to the\\nsearch index return the most relevant information for a writer to use when\\nwriting marketing articles.\\n\\n# Context\\nUse the follow contex to provide a set of specialized queries to the search index:\\n\\ntell me about your hiking jackets\\n\\n# Response format\\nThe response format is a JSON array that contains a list of specialized queries \\nto make to the search index. Here is an example:\\n\\ncontext: Can you find a selection of outdoor apparel?\\nqueries:\\n[\\n  \\\"outdoor apparel\\\",\\n  \\\"outdoor clothing\\\",\\n  \\\"outdoor gear\\\",\\n  \\\"outdoor clothing brands\\\",\\n  \\\"outdoor clothing stores\\\",\\n]\\n\\nThis only an example of the output structure. You should make sure to use the context the user gives you\\nto generate the queries.\\n\\n# Output format\\nOnly output the full array of specialized queries to make to the search index. Limit\\nyoursef to 5 queries.\"\n",
      "        },\n",
      "        {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": \"tell me about your hiking jackets\"\n",
      "        }\n",
      "    ],\n",
      "    \"max_tokens\": 1500\n",
      "}\n",
      "Ending create\n",
      "result:\n",
      "{\n",
      "    \"exception\": {\n",
      "        \"type\": \"<class 'openai.NotFoundError'>\",\n",
      "        \"traceback\": [\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 135, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/core.py\\\", line 335, in __call__\\n    return self.invoke(data)\\n           ^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/azure/executor.py\\\", line 88, in invoke\\n    response = client.chat.completions.create(**args)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py\\\", line 279, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions.py\\\", line 859, in create\\n    return self._post(\\n           ^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\\\", line 1283, in post\\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\\\", line 960, in request\\n    return self._request(\\n           ^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\\\", line 1064, in _request\\n    raise self._make_status_error_from_response(err.response) from None\\n\"\n",
      "        ],\n",
      "        \"message\": \"Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\",\n",
      "        \"args\": \"(\\\"Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\\\",)\"\n",
      "    }\n",
      "}\n",
      "Ending AzureOpenAIExecutor\n",
      "result:\n",
      "{\n",
      "    \"exception\": {\n",
      "        \"type\": \"<class 'openai.NotFoundError'>\",\n",
      "        \"traceback\": [\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 135, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/__init__.py\\\", line 335, in run\\n    result = executor(content)\\n             ^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 153, in wrapper\\n    raise e\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 135, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/core.py\\\", line 335, in __call__\\n    return self.invoke(data)\\n           ^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/azure/executor.py\\\", line 88, in invoke\\n    response = client.chat.completions.create(**args)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py\\\", line 279, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions.py\\\", line 859, in create\\n    return self._post(\\n           ^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\\\", line 1283, in post\\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\\\", line 960, in request\\n    return self._request(\\n           ^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\\\", line 1064, in _request\\n    raise self._make_status_error_from_response(err.response) from None\\n\"\n",
      "        ],\n",
      "        \"message\": \"Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\",\n",
      "        \"args\": \"(\\\"Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\\\",)\"\n",
      "    }\n",
      "}\n",
      "Ending run\n",
      "result:\n",
      "{\n",
      "    \"exception\": {\n",
      "        \"type\": \"<class 'openai.NotFoundError'>\",\n",
      "        \"traceback\": [\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 135, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/__init__.py\\\", line 400, in execute\\n    result = run(prompt, content, configuration, parameters, raw)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 153, in wrapper\\n    raise e\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 135, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/__init__.py\\\", line 335, in run\\n    result = executor(content)\\n             ^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 153, in wrapper\\n    raise e\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 135, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/core.py\\\", line 335, in __call__\\n    return self.invoke(data)\\n           ^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/azure/executor.py\\\", line 88, in invoke\\n    response = client.chat.completions.create(**args)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py\\\", line 279, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions.py\\\", line 859, in create\\n    return self._post(\\n           ^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\\\", line 1283, in post\\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\\\", line 960, in request\\n    return self._request(\\n           ^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\\\", line 1064, in _request\\n    raise self._make_status_error_from_response(err.response) from None\\n\"\n",
      "        ],\n",
      "        \"message\": \"Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\",\n",
      "        \"args\": \"(\\\"Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\\\",)\"\n",
      "    }\n",
      "}\n",
      "Ending execute\n",
      "result:\n",
      "{\n",
      "    \"exception\": {\n",
      "        \"type\": \"<class 'openai.NotFoundError'>\",\n",
      "        \"traceback\": [\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 135, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/workspaces/contoso-chat/src/api/contoso_chat/chat_request.py\\\", line 43, in get_response\\n    context = product.find_products(question)\\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/workspaces/contoso-chat/src/api/contoso_chat/product/product.py\\\", line 85, in find_products\\n    queries = prompty.execute(\\n              ^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 153, in wrapper\\n    raise e\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 135, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/__init__.py\\\", line 400, in execute\\n    result = run(prompt, content, configuration, parameters, raw)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 153, in wrapper\\n    raise e\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 135, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/__init__.py\\\", line 335, in run\\n    result = executor(content)\\n             ^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 153, in wrapper\\n    raise e\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/tracer.py\\\", line 135, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/core.py\\\", line 335, in __call__\\n    return self.invoke(data)\\n           ^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/prompty/azure/executor.py\\\", line 88, in invoke\\n    response = client.chat.completions.create(**args)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py\\\", line 279, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions.py\\\", line 859, in create\\n    return self._post(\\n           ^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\\\", line 1283, in post\\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\\\", line 960, in request\\n    return self._request(\\n           ^^^^^^^^^^^^^^\\n\",\n",
      "            \"  File \\\"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\\\", line 1064, in _request\\n    raise self._make_status_error_from_response(err.response) from None\\n\"\n",
      "        ],\n",
      "        \"message\": \"Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\",\n",
      "        \"args\": \"(\\\"Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\\\",)\"\n",
      "    }\n",
      "}\n",
      "Ending get_response\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      4\u001b[0m    test_data_df \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[0;32m----> 5\u001b[0m    response_results \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_response_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m    result_evaluated \u001b[38;5;241m=\u001b[39m evaluate()\n\u001b[1;32m      7\u001b[0m    create_summary(result_evaluated)\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36mcreate_response_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      6\u001b[0m question \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Run contoso-chat/chat_request flow to get response\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustomerId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustomerId\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Add results to list\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/prompty/tracer.py:153\u001b[0m, in \u001b[0;36m_trace_sync.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    138\u001b[0m     trace(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    140\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m         },\n\u001b[1;32m    152\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/prompty/tracer.py:135\u001b[0m, in \u001b[0;36m_trace_sync.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, inputs)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m, _results(result))\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/workspaces/contoso-chat/src/api/contoso_chat/chat_request.py:43\u001b[0m, in \u001b[0;36mget_response\u001b[0;34m(customerId, question, chat_history)\u001b[0m\n\u001b[1;32m     41\u001b[0m customer \u001b[38;5;241m=\u001b[39m get_customer(customerId)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mproduct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_products\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproducts complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetting result...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/contoso-chat/src/api/contoso_chat/product/product.py:85\u001b[0m, in \u001b[0;36mfind_products\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_products\u001b[39m(context: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28many\u001b[39m]:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Get product queries\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     model_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure_endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_OPENAI_ENDPOINT\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_OPENAI_API_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     84\u001b[0m     }\n\u001b[0;32m---> 85\u001b[0m     queries \u001b[38;5;241m=\u001b[39m \u001b[43mprompty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduct.prompty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     qs \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(queries)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Generate embeddings\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/prompty/tracer.py:153\u001b[0m, in \u001b[0;36m_trace_sync.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    138\u001b[0m     trace(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    140\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m         },\n\u001b[1;32m    152\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/prompty/tracer.py:135\u001b[0m, in \u001b[0;36m_trace_sync.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, inputs)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m, _results(result))\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/prompty/__init__.py:400\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(prompt, configuration, parameters, inputs, raw, config_name)\u001b[0m\n\u001b[1;32m    397\u001b[0m content \u001b[38;5;241m=\u001b[39m prepare(prompt, inputs)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# run LLM model\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/prompty/tracer.py:153\u001b[0m, in \u001b[0;36m_trace_sync.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    138\u001b[0m     trace(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    140\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m         },\n\u001b[1;32m    152\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/prompty/tracer.py:135\u001b[0m, in \u001b[0;36m_trace_sync.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, inputs)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m, _results(result))\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/prompty/__init__.py:335\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(prompt, content, configuration, parameters, raw)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# execute\u001b[39;00m\n\u001b[1;32m    334\u001b[0m executor \u001b[38;5;241m=\u001b[39m InvokerFactory\u001b[38;5;241m.\u001b[39mcreate_executor(invoker_type, prompt)\n\u001b[0;32m--> 335\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# skip?\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# invoker registration check\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/prompty/tracer.py:153\u001b[0m, in \u001b[0;36m_trace_sync.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    138\u001b[0m     trace(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    140\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m         },\n\u001b[1;32m    152\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/prompty/tracer.py:135\u001b[0m, in \u001b[0;36m_trace_sync.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, inputs)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m, _results(result))\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/prompty/core.py:335\u001b[0m, in \u001b[0;36mInvoker.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;129m@trace\u001b[39m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: \u001b[38;5;28many\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28many\u001b[39m:\n\u001b[1;32m    323\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Method to call the invoker\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m        The invoked\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/prompty/azure/executor.py:88\u001b[0m, in \u001b[0;36mAzureOpenAIExecutor.invoke\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     82\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment,\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [data],\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters,\n\u001b[1;32m     86\u001b[0m     }\n\u001b[1;32m     87\u001b[0m     trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, args)\n\u001b[0;32m---> 88\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    857\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1073\u001b[0m )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}"
     ]
    }
   ],
   "source": [
    "# create main funciton for python script\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "   test_data_df = load_data()\n",
    "   response_results = create_response_data(test_data_df)\n",
    "   result_evaluated = evaluate()\n",
    "   create_summary(result_evaluated)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
