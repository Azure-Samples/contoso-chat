{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Local Evaluation - Groundedness\n",
    "\n",
    "After you have setup and configured the prompt flow, its time to evaluation its performance. Here we can use the prompt flow SDK to test different questions and see how the prompt flow performs using the evaluation prompt flows provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow import PFClient\n",
    "from evaluate import run_local_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_client_local = PFClient()\n",
    "question = \"Can you tell me about your jackets?\"\n",
    "flow=\"../contoso-chat\" # Path to the flow directory\n",
    "inputs={ # Inputs to the flow\n",
    "    \"chat_history\": [],\n",
    "    \"question\": question,\n",
    "    \"customerId\": \"4\",\n",
    "}\n",
    "output = run_local_flow(flow, inputs, pf_client_local)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"answer\"] = \"\".join(list(output[\"answer\"]))\n",
    "output[\"answer\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the groundedness of the prompt flow with the answer from the above question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow=\"groundedness\"\n",
    "inputs={\n",
    "    \"question\": question,\n",
    "    \"context\": str(output[\"context\"]),\n",
    "    \"answer\": output[\"answer\"],\n",
    "}\n",
    "\n",
    "test = run_local_flow(flow, inputs, pf_client_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Evaluation - Multiple Metrics \n",
    "\n",
    "Now use the same prompt flow and test it against the Multi Evaluation flow for groundedness, coherence, fluency, and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = \"multi_flow\"\n",
    "inputs={\n",
    "    \"question\": question,\n",
    "    \"context\": str(output[\"context\"]),\n",
    "    \"answer\": output[\"answer\"],\n",
    "}\n",
    "test_multi = run_local_flow(flow, inputs, pf_client_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
