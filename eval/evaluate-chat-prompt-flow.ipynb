{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Evaluation - Groundedness\n",
    "\n",
    "After you have setup and configured the prompt flow, its time to evaluation its performance. Here we can use the prompt flow SDK to test different questions and see how the prompt flow performs using the evaluation prompt flows provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow import PFClient\n",
    "\n",
    "pf_client = PFClient()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a question to test the base prompt flow.\n",
    "question = \"Can you tell me about your jackets?\"\n",
    "customerId = \"4\"\n",
    "output = pf_client.test(\n",
    "    flow=\"../contoso-chat\", # Path to the flow directory\n",
    "    inputs={ # Inputs to the flow\n",
    "        \"chat_history\": [],\n",
    "        \"question\": question,\n",
    "        \"customerId\": customerId,\n",
    "    },\n",
    ")\n",
    "\n",
    "output[\"answer\"] = \"\".join(list(output[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the groundedness of the prompt flow with the answer from the above question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pf_client.test(\n",
    "    flow=\"groundedness\",\n",
    "    inputs={\n",
    "        \"question\": question,\n",
    "        \"context\": str(output[\"context\"]),\n",
    "        \"answer\": output[\"answer\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Evaluation - Multiple Metrics \n",
    "\n",
    "Now use the same prompt flow and test it against the Multi Evaluation flow for groundedness, coherence, fluency, and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multi = pf_client.test(\n",
    "    \"multi_flow\",\n",
    "    inputs={\n",
    "        \"question\": question,\n",
    "        \"context\": str(output[\"context\"]),\n",
    "        \"answer\": output[\"answer\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Studio Azure batch run on an evaluation json dataset\n",
    "\n",
    "Now in order to test these more thoroughly, we can use the Azure AI Studio to run batches of test data with the evaluation prompt flow on a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Import required libraries\n",
    "from promptflow.azure import PFClient\n",
    "\n",
    "# Import required libraries\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the `config.json` file with the subscription_id, resource_group, and workspace_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../config.json\"\n",
    "pf_azure_client = PFClient.from_config(credential=credential, path=config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the runtime from the AI Studio that will be used for the cloud batch runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the runtime to the name of the runtime you created previously\n",
    "runtime = \"automatic\"\n",
    "# load flow\n",
    "flow = \"../contoso-chat\"\n",
    "# load data\n",
    "data = \"../data/salestestdata.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current time stamp for run name\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "run_name = timestamp+\"chat_base_run\"\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a base run to use as the variant for the evaluation runs. \n",
    "\n",
    "_NOTE: If you get \"'An existing connection was forcibly closed by the remote host'\" run the cell again._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create base run in Azure Ai Studio\n",
    "base_run = pf_azure_client.run(\n",
    "    flow=flow,\n",
    "    data=data,\n",
    "    column_mapping={\n",
    "        # reference data\n",
    "        \"customerId\": \"${data.customerId}\",\n",
    "        \"question\": \"${data.question}\",\n",
    "    },\n",
    "    runtime=runtime,\n",
    "    # create a display name as current datetime\n",
    "    display_name=run_name,\n",
    "    name=run_name\n",
    ")\n",
    "print(base_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_azure_client.stream(base_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = pf_azure_client.get_details(base_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Eval run on Json Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_flow = \"multi_flow/\"\n",
    "data = \"../data/salestestdata.jsonl\"\n",
    "run_name = timestamp+\"chat_eval_run\"\n",
    "print(run_name)\n",
    "\n",
    "eval_run_variant = pf_azure_client.run(\n",
    "    flow=eval_flow,\n",
    "    data=data,  # path to the data file\n",
    "    run=base_run,  # use run as the variant\n",
    "    column_mapping={\n",
    "        # reference data\n",
    "        \"customerId\": \"${data.customerId}\",\n",
    "        \"question\": \"${data.question}\",\n",
    "        \"context\": \"${run.outputs.context}\",\n",
    "        # reference the run's output\n",
    "        \"answer\": \"${run.outputs.answer}\",\n",
    "    },\n",
    "    runtime=runtime,\n",
    "    display_name=run_name,\n",
    "    name=run_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_azure_client.stream(eval_run_variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = pf_azure_client.get_details(eval_run_variant)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = pf_azure_client.get_metrics(eval_run_variant)\n",
    "print(json.dumps(metrics, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_azure_client.visualize([base_run, eval_run_variant])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
