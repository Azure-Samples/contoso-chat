{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch run contoso-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow import PFClient\n",
    "from promptflow.entities import Run\n",
    "\n",
    "pf_client = PFClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load flow\n",
    "flow = \"../contoso-chat\"\n",
    "# load data\n",
    "data = \"../data/salestestdata.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current time stamp for run name\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "run_name = timestamp+\"chat_base_run\"\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a pf client to manage runs\n",
    "pf = PFClient()\n",
    "\n",
    "# Initialize an Run object\n",
    "base_run = Run(\n",
    "    flow=flow,\n",
    "    # run flow against local data or existing run, only one of data & run can be specified.\n",
    "    data=data,\n",
    "    #run=\"<existing-run-name>\",\n",
    "    column_mapping={\n",
    "        # reference data\n",
    "        \"customerId\": \"${data.customerId}\",\n",
    "        \"question\": \"${data.question}\",\n",
    "    },\n",
    "    #variant=\"${summarize_text_content.variant_0}\"\n",
    ")\n",
    "\n",
    "# Create the run\n",
    "result = pf.runs.create_or_update(base_run)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pf_client.get_details(base_run.name, all_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run contoso-chat evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.generative.evaluate import evaluate\n",
    "import json\n",
    "import numpy as np\n",
    "from azure.ai.resources.client import AIClient\n",
    "from azure.identity import InteractiveBrowserCredential, DefaultAzureCredential\n",
    "from azure.ai.resources.entities import AzureOpenAIModelConfiguration\n",
    "from azure.ai.generative.evaluate.metrics import PromptMetric\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#custom_prompt_metric = PromptMetric.from_template(path=\"metrics/my_custom_relevance.jinja2\", name=\"my_custom_relevance\")\n",
    "client = AIClient.from_config(credential=credential, path=config_path)\n",
    "tracking_uri = client.tracking_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = client.connections.get(\"aoai-connection\")\n",
    "\n",
    "config = AzureOpenAIModelConfiguration.from_connection(connection, model_name=\"gpt-4\",\n",
    "                                                           deployment_name=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(  # This will log metric/artifacts using mlflow\n",
    "    evaluation_name=\"contoso-chat-sales-only-eval\",\n",
    "    data=result_df,\n",
    "    task_type=\"qa\",\n",
    "    #metrics_list=[custom_prompt_metric, \"gpt_groundedness\", answer_length],\n",
    "    metrics_list=[\"gpt_groundedness\", \"gpt_relevance\", \"gpt_coherence\", \"gpt_fluency\"],\n",
    "    model_config=config,\n",
    "    data_mapping={\n",
    "        # reference data\n",
    "        \"customerId\": \"${inputs.customerId}\",\n",
    "        \"question\": \"${inputs.question}\",\n",
    "        \"contexts\": \"${outputs.context}\",\n",
    "        # reference the run's output\n",
    "        \"answer\": \"${outputs.answer}\",\n",
    "    },\n",
    "    tracking_uri=tracking_uri,\n",
    "    output_path=os.getcwd() + \"/downloaded_artifacts/remote\"\n",
    ")\n",
    "print(result)\n",
    "print(result.metrics_summary)\n",
    "print(\"Studio Url\")\n",
    "print(result.studio_url)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contoso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
