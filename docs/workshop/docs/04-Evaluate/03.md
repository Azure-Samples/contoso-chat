# 4.3 Run Batch Evaluation

In the previous section, we assessed a single answer for a single metric, running one Prompty at a time. In reality, we will need to run assessments automatically across a large set of test inputs, with all custom evaluators, before we can judge if the application is ready for production use. In this exercise, we'll run a batch evaluation on our Contoso Chat application, using a Jupyter notebook.

## 1. Run Evaluation Notebook

Navigate to the `src/api` folder in Visual Studio Code.

- Click: `evaluate-chat-flow.ipynb` - see: A Jupyter notebook
- Click: Select Kernel - choose "Python Environments" - pick recommended `Python 3.11.x`
- Click: `Run all` - this kickstarts the multi-step evaluation flow.

!!! warning "Troubleshooting: Evaluation gives an error message in the Notebook"

    On occasion, the evaluation notebook may throw an error after a few iterations. This is typically a transient error. To fix it, `Clear outputs` in the Jupyter Notebook, then `Restart` the kernel. `Run All` should complete the run this time.


## 2. Watch Evaluation Runs

One of the benefits of using Prompty is the built-in `Tracer` feature that captures execution traces for the entire workflow. These trace _runs_ are stored in  `.tracy` files in the `api/.runs/` folder as shown in the figure below.

- Keep this explorer sidebar open while the evaluation notebook runs/
- You see: `get_response` traces when our chat application is running
- You see: `groundedness` traces when its groundeness is evaluated
- You see: similar `fluency`, `coherence` and `relevance` traces

![Eval](./../img/Evaluation%20Runs.png)

## 3. Explore: Evaluation Trace

Click on any of these `.tracy` files to launch the _Trace Viewer_ window seen at right. 

- Note that this may take a while to appear. 
- You may need to click several runs before seeing a full trace.

Once the trace file is displayed, explore the panel to get an intuition for usage

- See: sequence of steps in orchestrated flow (left)
- See: prompt files with `load-prepare-run` sub-traces
- See: Azure OpenAIExecutor traces on model use
- Click: any trace to see its timing and details in pane (right)

!!! info "Want to learn more about Prompty Tracing? [Explore the documentation](https://github.com/microsoft/prompty/tree/main/runtime/prompty#using-tracing-in-prompty) to learn how to configure your application for traces, and how to view and publish traces for debugging and observability."

