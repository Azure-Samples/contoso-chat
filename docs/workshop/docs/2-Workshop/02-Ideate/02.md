# 3.2: Update Prompt Metadata


!!! quote "CUSTOMIZE YOUR PROMPTY ASSET FRONTMATTER"

    The Prompty asset uses a YAML format with **frontmatter** in the top half that specifies configuration metadata, and the **template** in the bottom half for specifying the prompt context, instructions and system persona. In this step we'll:

    1. Fix the deployment configuration error in the frontmatter
    1. Customize the frontmatter metadata to reflect our application
    1. Run the customized prompty asset to validate our changes

---


## 1. Update model configuration

1. Let's start by fixing the model deployment issue.
1. First, copy `basic.prompty` to `chat-0.prompty`. This helps us mimic the iterative nature of prompt engineering with each version of the asset getting us closer to the final prototype.
    ```title=""
    cp basic.prompty chat-0.prompty
    ```
1. Open the `chat-0.prompty` file in VS Code. The model configuration section looks like this:

    ```yaml title="chat-0.prompty"
    ---
    name: ExamplePrompt
    description: A prompt that uses context to ground an incoming question
    authors:
        - Seth Juarez
    model:
        api: chat
        configuration:
            type: azure_openai
            azure_endpoint: ${env:AZURE_OPENAI_ENDPOINT}
            azure_deployment: <your-deployment>
            api_version: 2024-07-01-preview
        parameters:
            max_tokens: 3000
    ```

1. Let's update it to fix the `azure_deployment` value. Update the `model` section as shown below and leave all other content untouched. _We recommend you copy-paste this directly into your file to avoid indentation errors._

    ```yaml title="chat-0.prompty"
    ---
    name: ExamplePrompt
    description: A prompt that uses context to ground an incoming question
    authors:
        - Seth Juarez
    model:
        api: chat
        configuration:
            type: azure_openai
            azure_endpoint: ${env:AZURE_OPENAI_ENDPOINT}
            azure_deployment: ${env:AZURE_OPENAI_CHAT_DEPLOYMENT}
            api_version: 2024-07-01-preview
        parameters:
            max_tokens: 3000
    ```

    !!! info "Prompty will use the AZURE_OPENAI_CHAT_DEPLOYMENT variable from the .env file we created earlier to find the OpenAI endpoint we pre-deployed. For now, that env specifies _gpt-35-turbo_ as the model."


## 2. Edit Basic information

Basic information about the prompt template is provided at the top of the file.

* **name**: Call this prompty `Contoso Chat Prompt`
* **description**: Use:
```
A retail assistant for Contoso Outdoors products retailer.
```
* **authors**: Replace the provided name with your own.

## 3. Edit the "sample" section

The **sample** section specifies the inputs to the prompty, and supplies default values to use if no input are provided. Edit that section as well.

* **firstName**: Choose any name other than your own (for example, `Nitya`).

* **context**: Remove this entire section. (We'll update this later)

* **question**: Replace the provided text with:
```
What can you tell me about your tents?
```

Your **sample** section should now look like this:
```
sample:
  firstName: Nitya
  question: What can you tell me about your tents?
```

## 4. Run updated Prompty file

1. Run `chat-0.prompty`. (Use the Run button or press F5.)

1. Check the OUTPUT pane. You will see a response something like this:
    - `"[info] Hey Nitya! Thank you for asking about our tents. ..."`

    !!! info "Generative AI models use randomness when creating responses, so your results aren't always the same." 


---

!!! success "CONGRATULATIONS. You updated your Prompty model configuration!"

**Continue ideating on your own!** If you like, try changing the `firstName` and `question` fields in the Prompty file and run it again. How do your changes affect the response?
