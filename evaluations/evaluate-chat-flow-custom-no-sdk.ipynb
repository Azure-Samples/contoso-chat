{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from promptflow.core import AzureOpenAIModelConfiguration, Prompty\n",
    "\n",
    "# Initialize Azure OpenAI Connection\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "        azure_deployment=\"gpt-4\",\n",
    "        api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../contoso_chat')  # Replace '/path/to/contoso_chat' with the actual path to the 'contoso_chat' folder\n",
    "\n",
    "from chat_request import get_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get output from data and save to results jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"../data/data.jsonl\"\n",
    "\n",
    "df = pd.read_json(data_path, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    customerId = row['customerId']\n",
    "    question = row['question']\n",
    "    \n",
    "    # Run contoso-chat/chat_request flow to get response\n",
    "    response = get_response(customerId=customerId, question=question, chat_history=[])\n",
    "    \n",
    "    # Add results to list\n",
    "    result = {\n",
    "        'question': question,\n",
    "        'context': response[\"context\"],\n",
    "        'answer': response[\"answer\"]\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# Save results to a JSONL file\n",
    "with open('result.jsonl', 'w') as file:\n",
    "    for result in results:\n",
    "        file.write(json.dumps(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load apology evaluatorfrom prompty\n",
    "groundedness_eval = Prompty.load(source=\"prompty/groundedness.prompty\", model={\"configuration\": model_config})\n",
    "fluency_eval = Prompty.load(source=\"prompty/fluency.prompty\", model={\"configuration\": model_config})\n",
    "coherence_eval = Prompty.load(source=\"prompty/coherence.prompty\", model={\"configuration\": model_config})\n",
    "relevance_eval = Prompty.load(source=\"prompty/relevance.prompty\", model={\"configuration\": model_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate results from results file\n",
    "results_path = 'result.jsonl'\n",
    "results = []\n",
    "with open(results_path, 'r') as file:\n",
    "    for line in file:\n",
    "        results.append(json.loads(line))\n",
    "\n",
    "for result in results:\n",
    "    question = result['question']\n",
    "    context = result['context']\n",
    "    answer = result['answer']\n",
    "    \n",
    "    groundedness_score = groundedness_eval(question=question, answer=answer, context=context)\n",
    "    fluency_score = fluency_eval(question=question, answer=answer, context=context)\n",
    "    coherence_score = coherence_eval(question=question, answer=answer, context=context)\n",
    "    relevance_score = relevance_eval(question=question, answer=answer, context=context)\n",
    "    \n",
    "    result['groundedness'] = groundedness_score\n",
    "    result['fluency'] = fluency_score\n",
    "    result['coherence'] = coherence_score\n",
    "    result['relevance'] = relevance_score\n",
    "\n",
    "# Save results to a JSONL file\n",
    "with open('result_evaluated.jsonl', 'w') as file:\n",
    "    for result in results:\n",
    "        file.write(json.dumps(result) + '\\n')\n",
    "\n",
    "# Print results\n",
    "\n",
    "df = pd.read_json('result_evaluated.jsonl', lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pf-prompty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
