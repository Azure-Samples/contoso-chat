{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "# Initialize Azure OpenAI Connection\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "        azure_deployment=\"gpt-4\",\n",
    "        api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"../data/data.jsonl\"\n",
    "\n",
    "df = pd.read_json(data_path, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.client import load_flow\n",
    "\n",
    "# load apology evaluatorfrom prompty\n",
    "groundedness_eval = load_flow(source=\"prompty/groundedness.prompty\", model={\"configuration\": model_config})\n",
    "fluency_eval = load_flow(source=\"prompty/fluency.prompty\", model={\"configuration\": model_config})\n",
    "coherence_eval = load_flow(source=\"prompty/coherence.prompty\", model={\"configuration\": model_config})\n",
    "relevance_eval = load_flow(source=\"prompty/relevance.prompty\", model={\"configuration\": model_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../contoso_chat')  # Replace '/path/to/contoso_chat' with the actual path to the 'contoso_chat' folder\n",
    "\n",
    "from chat_request import get_response\n",
    "from promptflow.evals.evaluators import RelevanceEvaluator\n",
    "\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "result_eval = evaluate(\n",
    "    data=\"../data/data.jsonl\",\n",
    "    target=get_response,\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval,\n",
    "        \"fluency\": fluency_eval,\n",
    "        \"coherence\": coherence_eval,\n",
    "        \"groundedness\": groundedness_eval,\n",
    "    },\n",
    "    # column mapping    return {\"question\": question, \"answer\": result, \"context\": context}\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"question\": \"${data.question}\",\n",
    "            \"answer\": \"${target.answer}\",\n",
    "            \"context\": \"${target.context}\",\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = pd.DataFrame(result_eval[\"rows\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save evaluation results to a JSONL file\n",
    "eval_result.to_json('eval_result.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pf-prompty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
