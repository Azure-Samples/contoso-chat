{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "# Initialize Azure OpenAI Connection\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "        azure_deployment=os.environ[\"AZURE_DEPLOYMENT_NAME\"],\n",
    "        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "        api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"../data/data.jsonl\"\n",
    "\n",
    "df = pd.read_json(data_path, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.client import load_flow\n",
    "\n",
    "# load apology evaluatorfrom prompty\n",
    "groundedness_eval = load_flow(source=\"prompty/groundedness.prompty\", model={\"configuration\": model_config})\n",
    "fluency_eval = load_flow(source=\"prompty/fluency.prompty\", model={\"configuration\": model_config})\n",
    "coherence_eval = load_flow(source=\"prompty/coherence.prompty\", model={\"configuration\": model_config})\n",
    "relevance_eval = load_flow(source=\"prompty/relevance.prompty\", model={\"configuration\": model_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../contoso_chat')  # Replace '/path/to/contoso_chat' with the actual path to the 'contoso_chat' folder\n",
    "\n",
    "from chat_request import get_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    customerId = row['customerId']\n",
    "    question = row['question']\n",
    "    \n",
    "    # Run contoso-chat/chat_request flow to get response\n",
    "    response = get_response(customerId=customerId, question=question, chat_history=[])\n",
    "    \n",
    "    # Add results to list\n",
    "    result = {\n",
    "        'customerId': customerId,\n",
    "        'question': question,\n",
    "        'response': response\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# Save results to a JSONL file\n",
    "with open('result.jsonl', 'w') as file:\n",
    "    for result in results:\n",
    "        file.write(json.dumps(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "#from promptflow.evals.evaluators.content_safty import violence_eval\n",
    "\n",
    "result_eval = evaluate(\n",
    "    data=\"result.jsonl\",\n",
    "    evaluators={\n",
    "        #\"violence\": violence_eval,\n",
    "        \"groundedness\": groundedness_eval,\n",
    "        \"fluency\": fluency_eval,\n",
    "        \"coherence\": coherence_eval,\n",
    "        \"relevence\": relevance_eval,\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "\n",
    "            \"question\": \"${response.question}\",\n",
    "            \"answer\": \"${response.answer}\",\n",
    "            \"context\": \"${response.context}\",\n",
    "\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = pd.DataFrame(result_eval[\"rows\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pf-prompty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
